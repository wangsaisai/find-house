import os
import json
import asyncio
import google.generativeai as genai
from typing import Dict, List, Any, Optional, Set
import logging
from dotenv import load_dotenv
import aiohttp
import time
import uuid
from datetime import datetime
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½® Google Gemini API
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("No GOOGLE_API_KEY found in environment variables.")
genai.configure(api_key=GEMINI_API_KEY)

# MCP æœåŠ¡å™¨é…ç½®
AMAP_MCP_KEY = os.getenv("AMAP_MCP_KEY")
AMAP_MCP_URL = f"https://mcp.amap.com/mcp?key={AMAP_MCP_KEY}"

class MCPToolManager:
    """é€šç”¨MCPå·¥å…·ç®¡ç†å™¨"""
    
    def __init__(self, url: str):
        self.url = url
        self.session = None
        self.request_id = 0
        self.available_tools = {}
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        await self.initialize()
        await self.load_available_tools()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def _next_id(self):
        self.request_id += 1
        return self.request_id
    
    async def initialize(self):
        """åˆå§‹åŒ– MCP è¿æ¥"""
        payload = {
            "jsonrpc": "2.0",
            "id": self._next_id(),
            "method": "initialize",
            "params": {
                "protocolVersion": "2024-11-05",
                "capabilities": {"tools": {}},
                "clientInfo": {
                    "name": "universal-travel-analyzer",
                    "version": "1.0.0"
                }
            }
        }
        
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json, text/event-stream"
        }
        
        async with self.session.post(self.url, json=payload, headers=headers) as response:
            if response.status != 200:
                raise Exception("MCP initialization failed")
            result = await response.json()
            return result
    
    async def load_available_tools(self):
        """åŠ è½½å¯ç”¨å·¥å…·åˆ—è¡¨"""
        payload = {
            "jsonrpc": "2.0",
            "id": self._next_id(),
            "method": "tools/list",
            "params": {}
        }
        
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json, text/event-stream"
        }
        
        async with self.session.post(self.url, json=payload, headers=headers) as response:
            if response.status == 200:
                result = await response.json()
                if "result" in result and "tools" in result["result"]:
                    for tool in result["result"]["tools"]:
                        self.available_tools[tool["name"]] = tool
                        logger.info(f"Loaded tool: {tool['name']}")
    
    async def call_tool(self, tool_name: str, arguments: dict):
        """è°ƒç”¨æŒ‡å®šçš„MCPå·¥å…·"""
        payload = {
            "jsonrpc": "2.0",
            "id": self._next_id(),
            "method": "tools/call",
            "params": {
                "name": tool_name,
                "arguments": arguments
            }
        }
        
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json, text/event-stream"
        }
        
        logger.info(f"Calling tool {tool_name} with arguments: {arguments}")
        async with self.session.post(self.url, json=payload, headers=headers) as response:
            if response.status != 200:
                text = await response.text()
                logger.error(f"Failed to call tool {tool_name}: {text}")
                return {"error": f"Failed to call tool {tool_name}"}
            result = await response.json()
            logger.info(f"Tool {tool_name} result received")
            return result
    
    def get_tools_description(self) -> str:
        """ç”Ÿæˆå·¥å…·æè¿°ï¼Œä¾›LLMç†è§£å¯ç”¨å·¥å…·"""
        descriptions = []
        for tool_name, tool_info in self.available_tools.items():
            desc = f"**{tool_name}**: {tool_info.get('description', 'æ— æè¿°')}"
            if 'inputSchema' in tool_info:
                schema = tool_info['inputSchema']
                if 'properties' in schema:
                    params = []
                    for param_name, param_info in schema['properties'].items():
                        param_desc = f"{param_name} ({param_info.get('type', 'unknown')})"
                        if param_info.get('description'):
                            param_desc += f": {param_info['description']}"
                        params.append(param_desc)
                    desc += f"\n  å‚æ•°: {', '.join(params)}"
            descriptions.append(desc)
        return "\n\n".join(descriptions)

class ConversationManager:
    """å¯¹è¯ç®¡ç†å™¨ï¼Œå¤„ç†å¤šè½®å¯¹è¯çŠ¶æ€"""
    
    def __init__(self):
        self.conversations = {}  # conversation_id -> conversation_data
        
    def create_conversation(self) -> str:
        """åˆ›å»ºæ–°å¯¹è¯"""
        conversation_id = str(uuid.uuid4())
        self.conversations[conversation_id] = {
            "id": conversation_id,
            "created_at": datetime.now().isoformat(),
            "messages": [],
            "context": {},
            "session_data": {}
        }
        return conversation_id
    
    def add_message(self, conversation_id: str, role: str, content: str, metadata: Dict = None):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯"""
        if conversation_id not in self.conversations:
            conversation_id = self.create_conversation()
        
        message = {
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }
        
        self.conversations[conversation_id]["messages"].append(message)
        return conversation_id
    
    def get_conversation_context(self, conversation_id: str) -> str:
        """è·å–å¯¹è¯ä¸Šä¸‹æ–‡"""
        if conversation_id not in self.conversations:
            return ""
        
        messages = self.conversations[conversation_id]["messages"]
        context_lines = []
        
        for msg in messages[-5:]:  # åªä¿ç•™æœ€è¿‘5æ¡æ¶ˆæ¯
            role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
            context_lines.append(f"{role}: {msg['content']}")
        
        return "\n".join(context_lines)

class UniversalTravelAnalyzer:
    """é€šç”¨æ™ºèƒ½å‡ºè¡Œåˆ†æå™¨"""
    
    def __init__(self):
        self.model = genai.GenerativeModel('gemini-2.0-flash-exp')
        self.conversation_manager = ConversationManager()
        self.scenario_templates = self._load_scenario_templates()
        
    def _load_scenario_templates(self) -> Dict[str, Dict]:
        """åŠ è½½åœºæ™¯æ¨¡æ¿"""
        return {
            "rental_housing": {
                "keywords": ["ç§Ÿæˆ¿", "æ‰¾æˆ¿", "ä½æˆ¿", "æˆ¿å­", "ç§Ÿèµ", "å±…ä½"],
                "required_tools": ["maps_geo", "maps_around_search", "maps_direction_transit_integrated"],
                "analysis_type": "ç§Ÿæˆ¿ä½ç½®åˆ†æ",
                "template": "rental_analysis"
            },
            "travel_planning": {
                "keywords": ["æ—…æ¸¸", "æ—…è¡Œ", "æ”»ç•¥", "æ™¯ç‚¹", "è¡Œç¨‹", "åº¦å‡"],
                "required_tools": ["maps_text_search", "maps_around_search"],
                "analysis_type": "æ—…æ¸¸è¡Œç¨‹è§„åˆ’",
                "template": "travel_planning"
            },
            "route_planning": {
                "keywords": ["è·¯çº¿", "å¯¼èˆª", "å‡ºè¡Œæ–¹å¼", "äº¤é€š", "åˆ°è¾¾"],
                "required_tools": ["maps_geo", "maps_direction_walking", "maps_direction_transit_integrated"],
                "analysis_type": "è·¯çº¿è§„åˆ’",
                "template": "route_planning"
            },
            "poi_search": {
                "keywords": ["é™„è¿‘", "å‘¨è¾¹", "æ‰¾", "æœç´¢", "æ¨è"],
                "required_tools": ["maps_around_search", "maps_text_search"],
                "analysis_type": "åœ°ç‚¹æœç´¢",
                "template": "poi_search"
            },
            "accommodation": {
                "keywords": ["é…’åº—", "ä½å®¿", "å®¢æ ˆ", "æ°‘å®¿", "å®¾é¦†"],
                "required_tools": ["maps_text_search", "maps_around_search"],
                "analysis_type": "ä½å®¿æ¨è",
                "template": "accommodation"
            }
        }
    
    async def analyze_request(self, query: str, context: Dict[str, Any] = None,
                            preferences: str = "", constraints: Dict[str, Any] = None) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·è¯·æ±‚"""
        start_time = time.time()
        
        # åˆ†ææŸ¥è¯¢æ„å›¾
        intent_analysis = await self.analyze_query_intent(query)
        analysis_type = intent_analysis.get("analysis_type", "general")
        
        # æ‰§è¡Œç›¸åº”çš„åˆ†ææµç¨‹
        async with MCPToolManager(AMAP_MCP_URL) as tool_manager:
            analysis_results = await self._execute_intelligent_analysis(
                tool_manager, query, intent_analysis, context or {}, preferences, constraints or {}
            )
        
        # æ·»åŠ å…ƒæ•°æ®
        analysis_results.update({
            "analysis_type": analysis_type,
            "processing_time": time.time() - start_time,
            "confidence_score": intent_analysis.get("confidence", 0.8),
            "data_sources": {"amap_api"}
        })
        
        return analysis_results
    
    async def analyze_query_intent(self, query: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢æ„å›¾"""
        intent_prompt = f"""
        è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾å’Œéœ€æ±‚ç±»å‹ï¼š

        ç”¨æˆ·æŸ¥è¯¢: "{query}"

        æ”¯æŒçš„åˆ†æç±»å‹å’Œå…³é”®è¯ï¼š
        1. ç§Ÿæˆ¿ä½ç½®åˆ†æ: ç§Ÿæˆ¿ã€æ‰¾æˆ¿ã€ä½æˆ¿ã€æˆ¿å­ã€ç§Ÿèµã€å±…ä½
        2. æ—…æ¸¸è¡Œç¨‹è§„åˆ’: æ—…æ¸¸ã€æ—…è¡Œã€æ”»ç•¥ã€æ™¯ç‚¹ã€è¡Œç¨‹ã€åº¦å‡
        3. è·¯çº¿è§„åˆ’: è·¯çº¿ã€å¯¼èˆªã€å‡ºè¡Œæ–¹å¼ã€äº¤é€šã€åˆ°è¾¾
        4. åœ°ç‚¹æœç´¢: é™„è¿‘ã€å‘¨è¾¹ã€æ‰¾ã€æœç´¢ã€æ¨è
        5. ä½å®¿æ¨è: é…’åº—ã€ä½å®¿ã€å®¢æ ˆã€æ°‘å®¿ã€å®¾é¦†

        è¯·åˆ†æå¹¶è¿”å›JSONæ ¼å¼ï¼š
        {{
            "analysis_type": "æœ€åŒ¹é…çš„åˆ†æç±»å‹",
            "confidence": 0.0-1.0,
            "key_entities": ["æå–çš„å…³é”®å®ä½“"],
            "location_info": ["æå–çš„åœ°ç‚¹ä¿¡æ¯"],
            "constraints": ["é¢„ç®—ã€æ—¶é—´ç­‰çº¦æŸ"],
            "recommended_tools": ["å»ºè®®ä½¿ç”¨çš„å·¥å…·"],
            "analysis_plan": ["åˆ†ææ­¥éª¤"]
        }}
        """
        
        try:
            response = self.model.generate_content(intent_prompt)
            
            # å°è¯•è§£æJSON
            result_text = response.text.strip()
            if result_text.startswith('```json'):
                result_text = result_text[7:-3].strip()
            elif result_text.startswith('```'):
                result_text = result_text[3:-3].strip()
            
            intent_result = json.loads(result_text)
            
            # åŒ¹é…åœºæ™¯æ¨¡æ¿
            for scenario_key, scenario_info in self.scenario_templates.items():
                if intent_result["analysis_type"] in scenario_info.get("analysis_type", ""):
                    intent_result["scenario"] = scenario_key
                    intent_result["recommended_tools"] = scenario_info["required_tools"]
                    break
            
            return intent_result
            
        except Exception as e:
            logger.error(f"Intent analysis failed: {e}")
            # è¿”å›é»˜è®¤åˆ†æ
            return {
                "analysis_type": "general",
                "confidence": 0.5,
                "key_entities": [query],
                "location_info": [],
                "constraints": [],
                "recommended_tools": ["maps_text_search"],
                "analysis_plan": ["general_search"]
            }
    
    async def _execute_intelligent_analysis(self, tool_manager: MCPToolManager, query: str,
                                          intent_analysis: Dict[str, Any], context: Dict[str, Any],
                                          preferences: str, constraints: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ™ºèƒ½åˆ†æ"""
        
        analysis_results = {
            "query": query,
            "intent_analysis": intent_analysis,
            "context": context,
            "preferences": preferences,
            "constraints": constraints,
            "tool_calls": [],
            "collected_data": {},
            "analysis_steps": []
        }
        
        # ç¬¬ä¸€æ­¥ï¼šåˆ¶å®šåˆ†æè®¡åˆ’
        planning_prompt = f"""
        ç”¨æˆ·æŸ¥è¯¢: "{query}"
        åˆ†æç±»å‹: {intent_analysis.get('analysis_type', 'general')}
        å…³é”®ä¿¡æ¯: {intent_analysis.get('key_entities', [])}
        åœ°ç‚¹ä¿¡æ¯: {intent_analysis.get('location_info', [])}
        ç”¨æˆ·åå¥½: {preferences}
        çº¦æŸæ¡ä»¶: {constraints}

        å¯ç”¨å·¥å…·:
        {tool_manager.get_tools_description()}

        è¯·åˆ¶å®šè¯¦ç»†çš„åˆ†æè®¡åˆ’ï¼Œå¹¶é€æ­¥æ‰§è¡Œã€‚ä½ éœ€è¦ï¼š
        1. ç¡®å®šéœ€è¦æ”¶é›†çš„ä¿¡æ¯ç±»å‹
        2. é€‰æ‹©åˆé€‚çš„å·¥å…·å’Œè°ƒç”¨é¡ºåº
        3. è¯´æ˜æ¯æ­¥çš„ç›®çš„

        è¯·å¼€å§‹åˆ†æå¹¶é€æ­¥æ‰§è¡Œã€‚
        """
        
        # ä½¿ç”¨LLMæŒ‡å¯¼çš„åˆ†ææµç¨‹
        max_iterations = 15
        iteration = 0
        
        while iteration < max_iterations:
            iteration += 1
            
            # è·å–å½“å‰çŠ¶æ€
            current_status = self._generate_analysis_status(analysis_results)
            
            # è¯¢é—®LLMä¸‹ä¸€æ­¥è¡ŒåŠ¨
            next_step_prompt = f"""
            å½“å‰åˆ†æçŠ¶æ€:
            {current_status}

            å·²æ‰§è¡Œçš„å·¥å…·è°ƒç”¨:
            {self._format_tool_calls_summary(analysis_results["tool_calls"])}

            å¯ç”¨å·¥å…·:
            {tool_manager.get_tools_description()}

            æ ¹æ®å½“å‰çŠ¶æ€å’Œç”¨æˆ·éœ€æ±‚: "{query}"

            è¯·å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š
            1. å¦‚æœéœ€è¦è°ƒç”¨å·¥å…·ï¼Œè¯·å›ç­”ï¼š
            ```
            CALL_TOOL
            å·¥å…·åç§°: tool_name
            å‚æ•°: {{"param": "value"}}
            åŸå› : è¯¦ç»†è¯´æ˜è°ƒç”¨åŸå› 
            ```

            2. å¦‚æœä¿¡æ¯æ”¶é›†å®Œæ¯•ï¼Œå¯ä»¥ç”Ÿæˆæœ€ç»ˆåˆ†æï¼Œè¯·å›ç­”ï¼š
            ```
            GENERATE_FINAL_RESPONSE
            åŸå› : è¯´æ˜ä¸ºä»€ä¹ˆå¯ä»¥ç”Ÿæˆæœ€ç»ˆå›ç­”
            ```

            3. å¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯ï¼Œè¯·å›ç­”ï¼š
            ```
            NEED_MORE_INFO
            éœ€è¦çš„ä¿¡æ¯: å…·ä½“æè¿°
            ```

            è¯·åˆ†æå¹¶å†³ç­–ä¸‹ä¸€æ­¥ã€‚
            """
            
            try:
                llm_decision = self.model.generate_content(next_step_prompt)
                decision_text = llm_decision.text.strip()
                
                logger.info(f"LLMå†³ç­– (ç¬¬{iteration}è½®): {decision_text}")
                
                # è§£æå†³ç­–
                if "CALL_TOOL" in decision_text:
                    tool_info = self._parse_tool_call_decision(decision_text)
                    if tool_info:
                        # æ‰§è¡Œå·¥å…·è°ƒç”¨
                        result = await tool_manager.call_tool(
                            tool_info["tool_name"], 
                            tool_info["arguments"]
                        )
                        
                        analysis_results["tool_calls"].append({
                            "iteration": iteration,
                            "tool_name": tool_info["tool_name"],
                            "arguments": tool_info["arguments"],
                            "result": result,
                            "reason": tool_info["reason"],
                            "success": "error" not in result
                        })
                        
                        # æ›´æ–°æ”¶é›†çš„æ•°æ®
                        self._update_collected_data(analysis_results, tool_info["tool_name"], result)
                        
                elif "GENERATE_FINAL_RESPONSE" in decision_text:
                    logger.info("LLMå†³å®šç”Ÿæˆæœ€ç»ˆå“åº”")
                    final_response = await self._generate_final_response(analysis_results)
                    analysis_results["final_response"] = final_response
                    break
                    
                elif "NEED_MORE_INFO" in decision_text:
                    logger.info(f"LLMéœ€è¦æ›´å¤šä¿¡æ¯: {decision_text}")
                    # å¯ä»¥åœ¨è¿™é‡Œå¤„ç†éœ€è¦ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯çš„æƒ…å†µ
                    
                else:
                    logger.warning(f"æ— æ³•è§£æLLMå†³ç­–: {decision_text}")
                    break
                    
            except Exception as e:
                logger.error(f"LLMå†³ç­–å¤„ç†å¤±è´¥: {e}")
                break
        
        return analysis_results
    
    def _parse_tool_call_decision(self, decision_text: str) -> Optional[Dict[str, Any]]:
        """è§£æLLMçš„å·¥å…·è°ƒç”¨å†³ç­–"""
        try:
            # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
            tool_name = None
            arguments = {}
            reason = ""
            
            lines = decision_text.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('å·¥å…·åç§°:'):
                    tool_name = line.split(':', 1)[1].strip()
                elif line.startswith('å‚æ•°:'):
                    param_str = line.split(':', 1)[1].strip()
                    try:
                        if param_str.startswith('{') and param_str.endswith('}'):
                            arguments = json.loads(param_str)
                        else:
                            arguments = {"query": param_str}
                    except json.JSONDecodeError:
                        arguments = {"query": param_str}
                elif line.startswith('åŸå› :'):
                    reason = line.split(':', 1)[1].strip()
            
            if tool_name:
                return {
                    "tool_name": tool_name,
                    "arguments": arguments,
                    "reason": reason
                }
                
        except Exception as e:
            logger.error(f"è§£æå·¥å…·è°ƒç”¨å†³ç­–å¤±è´¥: {e}")
        
        return None
    
    def _update_collected_data(self, analysis_results: Dict[str, Any], 
                             tool_name: str, result: Dict[str, Any]):
        """æ›´æ–°æ”¶é›†çš„æ•°æ®"""
        if "error" in result:
            return
            
        data_type = self._get_data_type_from_tool(tool_name)
        if data_type not in analysis_results["collected_data"]:
            analysis_results["collected_data"][data_type] = []
        
        analysis_results["collected_data"][data_type].append(result)
    
    def _get_data_type_from_tool(self, tool_name: str) -> str:
        """æ ¹æ®å·¥å…·åç§°ç¡®å®šæ•°æ®ç±»å‹"""
        if "geo" in tool_name:
            return "coordinates"
        elif "direction" in tool_name:
            return "routes"
        elif "around_search" in tool_name:
            return "nearby_pois"
        elif "text_search" in tool_name:
            return "search_results"
        else:
            return "other_data"
    
    def _generate_analysis_status(self, analysis_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆå½“å‰åˆ†æçŠ¶æ€æè¿°"""
        status_parts = []
        
        status_parts.append(f"ç”¨æˆ·æŸ¥è¯¢: {analysis_results['query']}")
        status_parts.append(f"åˆ†æç±»å‹: {analysis_results['intent_analysis'].get('analysis_type', 'unknown')}")
        
        # å·¥å…·è°ƒç”¨ç»Ÿè®¡
        tool_calls = analysis_results["tool_calls"]
        if tool_calls:
            status_parts.append(f"å·²æ‰§è¡Œå·¥å…·è°ƒç”¨: {len(tool_calls)}æ¬¡")
            successful_calls = [c for c in tool_calls if c.get("success", False)]
            status_parts.append(f"æˆåŠŸè°ƒç”¨: {len(successful_calls)}æ¬¡")
        else:
            status_parts.append("å°šæœªæ‰§è¡Œä»»ä½•å·¥å…·è°ƒç”¨")
        
        # æ•°æ®æ”¶é›†çŠ¶æ€
        collected_data = analysis_results["collected_data"]
        if collected_data:
            status_parts.append("å·²æ”¶é›†æ•°æ®ç±»å‹:")
            for data_type, data_list in collected_data.items():
                status_parts.append(f"  - {data_type}: {len(data_list)}æ¡è®°å½•")
        else:
            status_parts.append("å°šæœªæ”¶é›†åˆ°ä»»ä½•æ•°æ®")
        
        return "\n".join(status_parts)
    
    def _format_tool_calls_summary(self, tool_calls: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–å·¥å…·è°ƒç”¨æ‘˜è¦"""
        if not tool_calls:
            return "æ— "
        
        summary_lines = []
        for i, call in enumerate(tool_calls, 1):
            status = "æˆåŠŸ" if call.get("success", False) else "å¤±è´¥"
            summary_lines.append(f"{i}. {call['tool_name']} - {status}")
            summary_lines.append(f"   åŸå› : {call.get('reason', 'æœªè¯´æ˜')}")
        
        return "\n".join(summary_lines)
    
    async def _generate_final_response(self, analysis_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæœ€ç»ˆå“åº”"""
        query = analysis_results["query"]
        analysis_type = analysis_results["intent_analysis"].get("analysis_type", "general")
        collected_data = analysis_results["collected_data"]
        preferences = analysis_results.get("preferences", "")
        constraints = analysis_results.get("constraints", {})
        
        # æ„å»ºè¯¦ç»†çš„æ•°æ®å†…å®¹ä¾›LLMåˆ†æ
        detailed_data = self._build_detailed_data_for_analysis(collected_data)
        
        # æ ¹æ®åˆ†æç±»å‹ä½¿ç”¨ä¸åŒçš„promptæ¨¡æ¿
        response_prompt = self._build_response_prompt_by_type(
            query, analysis_type, detailed_data, preferences, constraints
        )
        
        try:
            generation_config = genai.types.GenerationConfig(
                temperature=0.1,  # é™ä½æ¸©åº¦æé«˜å‡†ç¡®æ€§
                candidate_count=1
            )
            
            response = self.model.generate_content(
                response_prompt,
                generation_config=generation_config
            )
            return response.text
        except Exception as e:
            logger.error(f"ç”Ÿæˆæœ€ç»ˆå“åº”å¤±è´¥: {e}")
            return self._generate_fallback_response(query, analysis_type, collected_data)
    
    def _build_detailed_data_for_analysis(self, collected_data: Dict[str, List]) -> str:
        """æ„å»ºè¯¦ç»†çš„æ•°æ®å†…å®¹ä¾›LLMåˆ†æ"""
        detailed_sections = []
        
        for data_type, data_list in collected_data.items():
            if not data_list:
                continue
                
            section = f"\n=== {data_type.upper()} æ•°æ® ({len(data_list)}æ¡) ==="
            
            for i, data_item in enumerate(data_list[:3], 1):  # é™åˆ¶æ¯ç±»æ•°æ®æœ€å¤š3æ¡ï¼Œé¿å…promptè¿‡é•¿
                try:
                    # æå–å…³é”®ä¿¡æ¯
                    key_info = self._extract_key_info_from_data(data_type, data_item)
                    section += f"\nç¬¬{i}æ¡æ•°æ®: {key_info}"
                except Exception as e:
                    section += f"\nç¬¬{i}æ¡æ•°æ®: æ•°æ®è§£æå¤±è´¥ - {str(e)}"
            
            detailed_sections.append(section)
        
        return "\n".join(detailed_sections) if detailed_sections else "æš‚æ— è¯¦ç»†æ•°æ®"
    
    def _extract_key_info_from_data(self, data_type: str, data_item: Dict) -> str:
        """ä»æ•°æ®é¡¹ä¸­æå–å…³é”®ä¿¡æ¯"""
        try:
            if data_type == "coordinates":
                return self._extract_coordinates_info(data_item)
            elif data_type == "routes":
                return self._extract_routes_info(data_item)
            elif data_type == "nearby_pois":
                return self._extract_pois_info(data_item)
            elif data_type == "search_results":
                return self._extract_search_info(data_item)
            else:
                return f"æ•°æ®ç±»å‹: {data_type}, å†…å®¹: {str(data_item)[:200]}..."
        except Exception as e:
            return f"æ•°æ®è§£æé”™è¯¯: {str(e)}"
    
    def _extract_coordinates_info(self, data_item: Dict) -> str:
        """æå–åæ ‡ä¿¡æ¯"""
        try:
            if "result" in data_item and "content" in data_item["result"]:
                content = data_item["result"]["content"]
                if content and isinstance(content, list) and content[0].get("text"):
                    geo_data = json.loads(content[0]["text"])
                    if "results" in geo_data and geo_data["results"]:
                        result = geo_data["results"][0]
                        location = result.get("location", "æœªçŸ¥åæ ‡")
                        formatted_address = result.get("formatted_address", "æœªçŸ¥åœ°å€")
                        city = result.get("city", "æœªçŸ¥åŸå¸‚")
                        return f"åœ°å€: {formatted_address}, åæ ‡: {location}, åŸå¸‚: {city}"
            return "åæ ‡ä¿¡æ¯è§£æå¤±è´¥"
        except Exception as e:
            return f"åæ ‡è§£æé”™è¯¯: {str(e)}"
    
    def _extract_routes_info(self, data_item: Dict) -> str:
        """æå–è·¯çº¿ä¿¡æ¯"""
        try:
            if "result" in data_item and "content" in data_item["result"]:
                content = data_item["result"]["content"]
                if content and isinstance(content, list) and content[0].get("text"):
                    route_data = json.loads(content[0]["text"])
                    
                    # æå–è·¯çº¿å…³é”®ä¿¡æ¯
                    if "routes" in route_data and route_data["routes"]:
                        route = route_data["routes"][0]
                        
                        # åŸºæœ¬ä¿¡æ¯
                        distance = route.get("distance", "æœªçŸ¥")
                        duration = route.get("duration", "æœªçŸ¥")
                        
                        # å…¬äº¤è·¯çº¿ä¿¡æ¯
                        if "transits" in route:
                            transit = route["transits"][0] if route["transits"] else {}
                            cost = transit.get("cost", "æœªçŸ¥è´¹ç”¨")
                            duration_text = f"{int(duration)//60}åˆ†é’Ÿ" if str(duration).isdigit() else duration
                            distance_text = f"{float(distance)/1000:.1f}å…¬é‡Œ" if str(distance).isdigit() else distance
                            
                            # æå–æ¢ä¹˜ä¿¡æ¯
                            segments = transit.get("segments", [])
                            route_desc = []
                            for segment in segments:
                                if "bus" in segment:
                                    bus_info = segment["bus"]
                                    buslines = bus_info.get("buslines", [])
                                    if buslines:
                                        line_name = buslines[0].get("name", "æœªçŸ¥çº¿è·¯")
                                        route_desc.append(f"ä¹˜å{line_name}")
                                elif "walking" in segment:
                                    walk_distance = segment["walking"].get("distance", "0")
                                    if int(walk_distance) > 100:  # åªæ˜¾ç¤ºè¶…è¿‡100ç±³çš„æ­¥è¡Œ
                                        route_desc.append(f"æ­¥è¡Œ{int(walk_distance)}ç±³")
                            
                            route_text = " â†’ ".join(route_desc) if route_desc else "è·¯çº¿è¯¦æƒ…è§£æä¸­"
                            return f"æ€»æ—¶é•¿: {duration_text}, æ€»è·ç¦»: {distance_text}, è´¹ç”¨: {cost}å…ƒ, è·¯çº¿: {route_text}"
                        
                        # æ­¥è¡Œè·¯çº¿ä¿¡æ¯
                        elif "paths" in route:
                            duration_text = f"{int(duration)//60}åˆ†é’Ÿ" if str(duration).isdigit() else duration
                            distance_text = f"{float(distance)/1000:.1f}å…¬é‡Œ" if str(distance).isdigit() else distance
                            return f"æ­¥è¡Œæ—¶é•¿: {duration_text}, è·ç¦»: {distance_text}"
                    
            return "è·¯çº¿ä¿¡æ¯è§£æå¤±è´¥"
        except Exception as e:
            return f"è·¯çº¿è§£æé”™è¯¯: {str(e)}"
    
    def _extract_pois_info(self, data_item: Dict) -> str:
        """æå–POIä¿¡æ¯"""
        try:
            if "result" in data_item and "content" in data_item["result"]:
                content = data_item["result"]["content"]
                if content and isinstance(content, list) and content[0].get("text"):
                    poi_data = json.loads(content[0]["text"])
                    
                    if "pois" in poi_data and poi_data["pois"]:
                        pois = poi_data["pois"][:5]  # åªå–å‰5ä¸ª
                        poi_list = []
                        for poi in pois:
                            name = poi.get("name", "æœªçŸ¥åç§°")
                            type_code = poi.get("type", "æœªçŸ¥ç±»å‹")
                            address = poi.get("address", "æœªçŸ¥åœ°å€")
                            distance = poi.get("distance", "æœªçŸ¥è·ç¦»")
                            if str(distance).isdigit():
                                distance = f"{int(distance)}ç±³"
                            poi_list.append(f"{name}({type_code}) - {address} - è·ç¦»{distance}")
                        
                        return f"æ‰¾åˆ°{len(poi_data['pois'])}ä¸ªåœ°ç‚¹: " + "; ".join(poi_list)
            
            return "POIä¿¡æ¯è§£æå¤±è´¥"
        except Exception as e:
            return f"POIè§£æé”™è¯¯: {str(e)}"
    
    def _extract_search_info(self, data_item: Dict) -> str:
        """æå–æœç´¢ç»“æœä¿¡æ¯"""
        try:
            if "result" in data_item and "content" in data_item["result"]:
                content = data_item["result"]["content"]
                if content and isinstance(content, list) and content[0].get("text"):
                    search_data = json.loads(content[0]["text"])
                    
                    if "pois" in search_data and search_data["pois"]:
                        count = len(search_data["pois"])
                        sample_names = [poi.get("name", "æœªçŸ¥") for poi in search_data["pois"][:3]]
                        return f"æœç´¢åˆ°{count}ä¸ªç»“æœï¼ŒåŒ…æ‹¬: {', '.join(sample_names)}ç­‰"
            
            return "æœç´¢ç»“æœè§£æå¤±è´¥"
        except Exception as e:
            return f"æœç´¢ç»“æœè§£æé”™è¯¯: {str(e)}"
    
    def _build_response_prompt_by_type(self, query: str, analysis_type: str, 
                                     detailed_data: str, preferences: str, 
                                     constraints: Dict) -> str:
        """æ ¹æ®åˆ†æç±»å‹æ„å»ºä¸åŒçš„å“åº”prompt"""
        
        base_info = f"""
        ç”¨æˆ·æŸ¥è¯¢: "{query}"
        åˆ†æç±»å‹: {analysis_type}
        ç”¨æˆ·åå¥½: {preferences}
        çº¦æŸæ¡ä»¶: {constraints}
        
        æ”¶é›†åˆ°çš„è¯¦ç»†æ•°æ®:
        {detailed_data}
        """
        
        if "è·¯çº¿è§„åˆ’" in analysis_type or "route" in analysis_type.lower():
            return f"""
            {base_info}
            
            è¯·åŸºäºä¸Šè¿°æ•°æ®ç”Ÿæˆè¯¦ç»†çš„è·¯çº¿è§„åˆ’åˆ†ææŠ¥å‘Šï¼Œå¿…é¡»åŒ…å«å…·ä½“ä¿¡æ¯ï¼š

            ## {query.replace('?', '').replace('ï¼Ÿ', '')}åˆ†ææŠ¥å‘Š

            **1. é’ˆå¯¹ç”¨æˆ·å…·ä½“éœ€æ±‚çš„åˆ†æ:**
            è¯¦ç»†åˆ†æç”¨æˆ·çš„å‡ºè¡Œéœ€æ±‚å’Œçº¦æŸæ¡ä»¶ã€‚

            **2. åŸºäºæ•°æ®çš„æ¨èå’Œå»ºè®®:**
            
            æ ¹æ®æ”¶é›†åˆ°çš„è·¯çº¿æ•°æ®ï¼Œæä¾›å…·ä½“çš„å‡ºè¡Œæ–¹æ¡ˆï¼š
            
            ### æ¨èæ–¹æ¡ˆ1: [å…·ä½“äº¤é€šæ–¹å¼]
            - **å‡ºè¡Œæ–¹å¼**: [å…¬äº¤/åœ°é“/æ­¥è¡Œ/ç»¼åˆ]
            - **æ€»æ—¶é•¿**: Xåˆ†é’Ÿ
            - **æ€»è·ç¦»**: X.Xå…¬é‡Œ  
            - **è´¹ç”¨**: Xå…ƒ
            - **è¯¦ç»†è·¯çº¿**: [å…·ä½“çš„æ¢ä¹˜æ­¥éª¤]
            - **ä¼˜åŠ¿**: [æ—¶é—´/è´¹ç”¨/ä¾¿åˆ©æ€§åˆ†æ]
            
            ### æ¨èæ–¹æ¡ˆ2: [å¤‡é€‰æ–¹æ¡ˆ]
            [ç±»ä¼¼è¯¦ç»†ä¿¡æ¯]
            
            ### æ–¹æ¡ˆå¯¹æ¯”:
            | æ–¹æ¡ˆ | æ—¶é•¿ | è´¹ç”¨ | æ¢ä¹˜æ¬¡æ•° | æ¨èæŒ‡æ•° |
            |------|------|------|----------|----------|
            | æ–¹æ¡ˆ1 | XXåˆ†é’Ÿ | XXå…ƒ | Xæ¬¡ | â­â­â­â­â­ |
            | æ–¹æ¡ˆ2 | XXåˆ†é’Ÿ | XXå…ƒ | Xæ¬¡ | â­â­â­â­ |

            **3. å®ç”¨çš„æ‰§è¡Œæ­¥éª¤:**
            1. å…·ä½“çš„å‡ºè¡Œæ­¥éª¤
            2. è´­ç¥¨/æ”¯ä»˜æ–¹å¼
            3. æ³¨æ„äº‹é¡¹

            **4. æ³¨æ„äº‹é¡¹å’Œæé†’:**
            - å®æ—¶äº¤é€šçŠ¶å†µ
            - ç­æ¬¡æ—¶é—´
            - å…¶ä»–é‡è¦æé†’

            è¯·ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯åŸºäºå®é™…æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œå¦‚æœæ•°æ®ä¸è¶³è¯·æ˜ç¡®è¯´æ˜ã€‚
            """
        
        elif "ç§Ÿæˆ¿" in analysis_type or "rental" in analysis_type.lower():
            return f"""
            {base_info}
            
            è¯·åŸºäºä¸Šè¿°æ•°æ®ç”Ÿæˆè¯¦ç»†çš„ç§Ÿæˆ¿ä½ç½®åˆ†ææŠ¥å‘Šï¼š

            ## ğŸ  ç§Ÿæˆ¿ä½ç½®åˆ†ææŠ¥å‘Š

            **1. é’ˆå¯¹ç”¨æˆ·å…·ä½“éœ€æ±‚çš„åˆ†æ:**
            [åˆ†æå·¥ä½œåœ°ç‚¹ã€é¢„ç®—ã€åå¥½ç­‰]

            **2. åŸºäºæ•°æ®çš„æ¨èå’Œå»ºè®®:**

            ### ğŸŒŸ æ¨èåŒºåŸŸ1: [å…·ä½“åŒºåŸŸåç§°]
            **æ¨èç†ç”±**: [è¯¦ç»†åˆ†æé€šå‹¤ä¾¿åˆ©æ€§]
            **åŒºåŸŸç‰¹ç‚¹**: [ç¯å¢ƒã€æˆ¿æºã€ç”Ÿæ´»æ°›å›´]
            **é¢„ä¼°ç§Ÿé‡‘**: [å…·ä½“ä»·æ ¼èŒƒå›´]
            
            #### ğŸš‡ é€šå‹¤åˆ†æ:
            - **åˆ°å·¥ä½œåœ°ç‚¹A**: [å…·ä½“è·¯çº¿] - Xåˆ†é’Ÿ, Xå…ƒ/å¤©
            - **åˆ°å·¥ä½œåœ°ç‚¹B**: [å…·ä½“è·¯çº¿] - Xåˆ†é’Ÿ, Xå…ƒ/å¤©
            
            #### ğŸ˜ï¸ å‘¨è¾¹è®¾æ–½:
            [åŸºäºPOIæ•°æ®çš„å…·ä½“è®¾æ–½ä¿¡æ¯]

            ### ğŸŒŸ æ¨èåŒºåŸŸ2: [ç¬¬äºŒä¸ªåŒºåŸŸ]
            [ç±»ä¼¼è¯¦ç»†ç»“æ„]

            **3. å®ç”¨çš„æ‰§è¡Œæ­¥éª¤:**
            [å…·ä½“çš„æ‰¾æˆ¿æ­¥éª¤]

            **4. æ³¨æ„äº‹é¡¹å’Œæé†’:**
            [å®ç”¨çš„ç§Ÿæˆ¿å»ºè®®]

            è¯·ç¡®ä¿æä¾›å…·ä½“ã€å¯æ‰§è¡Œçš„å»ºè®®ã€‚
            """
        
        elif "æ—…æ¸¸" in analysis_type or "travel" in analysis_type.lower():
            return f"""
            {base_info}
            
            è¯·åŸºäºä¸Šè¿°æ•°æ®ç”Ÿæˆè¯¦ç»†çš„æ—…æ¸¸è¡Œç¨‹è§„åˆ’æŠ¥å‘Šï¼š

            ## âœˆï¸ æ—…æ¸¸è¡Œç¨‹è§„åˆ’æŠ¥å‘Š

            **1. é’ˆå¯¹ç”¨æˆ·å…·ä½“éœ€æ±‚çš„åˆ†æ:**
            [åˆ†ææ—…æ¸¸ç›®çš„åœ°ã€æ—¶é—´ã€é¢„ç®—ã€åå¥½ç­‰]

            **2. åŸºäºæ•°æ®çš„æ¨èå’Œå»ºè®®:**

            ### ğŸ“… Day 1: [å…·ä½“å®‰æ’]
            - **ä¸Šåˆ**: [å…·ä½“æ™¯ç‚¹] - [æ¸¸ç©æ—¶é—´] - [äº¤é€šæ–¹å¼]
            - **ä¸‹åˆ**: [å…·ä½“å®‰æ’]
            - **æ™šä¸Š**: [ä½å®¿/ç¾é£Ÿæ¨è]
            - **é¢„ç®—**: Xå…ƒ

            ### ğŸ“… Day 2: [å…·ä½“å®‰æ’]
            [ç±»ä¼¼è¯¦ç»†å®‰æ’]

            ### ğŸ½ï¸ ç¾é£Ÿæ¨è:
            [åŸºäºæœç´¢æ•°æ®çš„å…·ä½“é¤å…æ¨è]

            ### ğŸ¨ ä½å®¿å»ºè®®:
            [å…·ä½“çš„ä½å®¿æ¨èå’Œä»·æ ¼]

            **3. å®ç”¨çš„æ‰§è¡Œæ­¥éª¤:**
            [é¢„è®¢æµç¨‹ã€å‡†å¤‡äº‹é¡¹]

            **4. æ³¨æ„äº‹é¡¹å’Œæé†’:**
            [å¤©æ°”ã€äº¤é€šã€å®‰å…¨ç­‰æé†’]

            è¯·æä¾›å…·ä½“å¯è¡Œçš„è¡Œç¨‹å®‰æ’ã€‚
            """
        
        else:
            # é€šç”¨æ¨¡æ¿
            return f"""
            {base_info}
            
            è¯·åŸºäºä¸Šè¿°æ•°æ®ç”Ÿæˆè¯¦ç»†ã€å®ç”¨çš„åˆ†ææŠ¥å‘Šï¼Œå¿…é¡»åŒ…å«ï¼š

            ## {analysis_type}åˆ†ææŠ¥å‘Š

            **1. é’ˆå¯¹ç”¨æˆ·å…·ä½“éœ€æ±‚çš„åˆ†æ:**
            [è¯¦ç»†åˆ†æç”¨æˆ·éœ€æ±‚]

            **2. åŸºäºæ•°æ®çš„æ¨èå’Œå»ºè®®:**
            [åŸºäºå®é™…æ”¶é›†æ•°æ®çš„å…·ä½“æ¨èï¼ŒåŒ…å«å…·ä½“æ•°å­—ã€åœ°ç‚¹ã€æ—¶é—´ç­‰]

            **3. å®ç”¨çš„æ‰§è¡Œæ­¥éª¤:**
            [å¯æ“ä½œçš„å…·ä½“æ­¥éª¤]

            **4. æ³¨æ„äº‹é¡¹å’Œæé†’:**
            [é‡è¦çš„æ³¨æ„äº‹é¡¹]

            è¯·ç¡®ä¿æä¾›å…·ä½“ã€å‡†ç¡®ã€å¯æ‰§è¡Œçš„ä¿¡æ¯ï¼Œé¿å…æ³›æ³›è€Œè°ˆã€‚
            """
    
    def _generate_fallback_response(self, query: str, analysis_type: str, 
                                  collected_data: Dict[str, List]) -> str:
        """ç”Ÿæˆå¤‡ç”¨å“åº”"""
        data_summary = []
        for data_type, data_list in collected_data.items():
            if data_list:
                data_summary.append(f"- {data_type}: {len(data_list)}æ¡æ•°æ®")
        
        return f"""
        ## {analysis_type}åˆ†ææŠ¥å‘Š

        **æŸ¥è¯¢å†…å®¹**: {query}

        **æ•°æ®æ”¶é›†çŠ¶å†µ**:
        {chr(10).join(data_summary) if data_summary else "- æš‚æœªæ”¶é›†åˆ°æ•°æ®"}

        **åˆ†æç»“æœ**:
        ç”±äºæŠ€æœ¯åŸå› ï¼Œæ— æ³•ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚å»ºè®®æ‚¨ï¼š

        1. æ£€æŸ¥ç½‘ç»œè¿æ¥çŠ¶æ€
        2. ç¡®è®¤æŸ¥è¯¢ä¿¡æ¯çš„å‡†ç¡®æ€§  
        3. ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ

        æˆ‘ä»¬å·²æ”¶é›†äº†ç›¸å…³æ•°æ®ï¼Œä½†åœ¨ç”Ÿæˆæœ€ç»ˆåˆ†ææ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·æä¾›æ›´å…·ä½“çš„éœ€æ±‚æè¿°æˆ–ç¨åé‡è¯•ã€‚
        """
    
    async def process_chat_message(self, message: str, conversation_id: Optional[str] = None) -> Dict[str, Any]:
        """å¤„ç†å¯¹è¯æ¶ˆæ¯"""
        if not conversation_id:
            conversation_id = self.conversation_manager.create_conversation()
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
        self.conversation_manager.add_message(conversation_id, "user", message)
        
        # è·å–å¯¹è¯ä¸Šä¸‹æ–‡
        context = self.conversation_manager.get_conversation_context(conversation_id)
        
        # åˆ†ææ¶ˆæ¯ç±»å‹
        if self._is_simple_question(message):
            # ç®€å•é—®ç­”ï¼Œä¸éœ€è¦è°ƒç”¨å·¥å…·
            response = await self._handle_simple_chat(message, context)
            chat_result = {
                "response": response,
                "conversation_id": conversation_id,
                "message_type": "simple_qa",
                "requires_action": False
            }
        else:
            # å¤æ‚åˆ†æï¼Œéœ€è¦è°ƒç”¨å·¥å…·
            analysis_result = await self.analyze_request(message)
            response = analysis_result.get("final_response", "åˆ†æå¤±è´¥ï¼Œè¯·é‡è¯•")
            chat_result = {
                "response": response,
                "conversation_id": conversation_id,
                "message_type": "analysis",
                "requires_action": True,
                "tools_used": [call.get("tool_name") for call in analysis_result.get("tool_calls", [])],
                "confidence": analysis_result.get("confidence_score", 0.8)
            }
        
        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å¯¹è¯å†å²
        self.conversation_manager.add_message(conversation_id, "assistant", response)
        
        return chat_result
    
    def _is_simple_question(self, message: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç®€å•é—®é¢˜"""
        simple_patterns = [
            r"^(ä½ å¥½|hello|hi)",
            r"^(è°¢è°¢|thank)",
            r"^(å†è§|bye)",
            r"ä½ æ˜¯|ä»€ä¹ˆæ˜¯|å¦‚ä½•ä½¿ç”¨",
            r"æ”¯æŒ.*å—",
            r"å¯ä»¥.*å—"
        ]
        
        for pattern in simple_patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return True
        return False
    
    async def _handle_simple_chat(self, message: str, context: str) -> str:
        """å¤„ç†ç®€å•å¯¹è¯"""
        chat_prompt = f"""
        å¯¹è¯ä¸Šä¸‹æ–‡:
        {context}

        ç”¨æˆ·æ¶ˆæ¯: {message}

        è¯·ä½œä¸ºä¸€ä¸ªæ™ºèƒ½å‡ºè¡ŒåŠ©æ‰‹ï¼Œç®€æ´å‹å¥½åœ°å›å¤ç”¨æˆ·ã€‚å¦‚æœç”¨æˆ·è¯¢é—®åŠŸèƒ½ï¼Œè¯·ä»‹ç»ä½ èƒ½å¸®åŠ©ç”¨æˆ·è¿›è¡Œï¼š
        - ç§Ÿæˆ¿ä½ç½®åˆ†æ
        - æ—…æ¸¸è¡Œç¨‹è§„åˆ’  
        - è·¯çº¿è§„åˆ’
        - åœ°ç‚¹æœç´¢
        - ä½å®¿æ¨è
        ç­‰å‡ºè¡Œç›¸å…³æœåŠ¡ã€‚
        """
        
        try:
            response = self.model.generate_content(chat_prompt)
            return response.text
        except Exception as e:
            logger.error(f"ç®€å•å¯¹è¯å¤„ç†å¤±è´¥: {e}")
            return "æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„æ™ºèƒ½å‡ºè¡ŒåŠ©æ‰‹ï¼Œå¯ä»¥å¸®æ‚¨åˆ†æç§Ÿæˆ¿ä½ç½®ã€è§„åˆ’æ—…æ¸¸è¡Œç¨‹ã€æœç´¢åœ°ç‚¹ç­‰ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨çš„éœ€æ±‚ï¼"
    
    def load_conversation_state(self, conversation_id: str, session_data: Dict[str, Any]):
        """åŠ è½½å¯¹è¯çŠ¶æ€"""
        if conversation_id not in self.conversation_manager.conversations:
            self.conversation_manager.conversations[conversation_id] = {
                "id": conversation_id,
                "created_at": datetime.now().isoformat(),
                "messages": [],
                "context": {},
                "session_data": session_data
            }
    
    async def get_system_capabilities(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿèƒ½åŠ›"""
        try:
            async with MCPToolManager(AMAP_MCP_URL) as tool_manager:
                available_tools = list(tool_manager.available_tools.keys())
        except:
            available_tools = ["åœ°å›¾å·¥å…·è¿æ¥å¤±è´¥"]
        
        return {
            "scenarios": list(self.scenario_templates.keys()),
            "tools": available_tools,
            "analysis_types": [template["analysis_type"] for template in self.scenario_templates.values()],
            "data_sources": ["é«˜å¾·åœ°å›¾API", "Gemini LLM"],
            "examples": [
                "æˆ‘åœ¨åŒ—äº¬æµ·æ·€åŒºå·¥ä½œï¼Œæƒ³æ‰¾æˆ¿å­",
                "å¸®æˆ‘è§„åˆ’æˆéƒ½3å¤©2å¤œæ—…æ¸¸æ”»ç•¥", 
                "ä»ä¸Šæµ·åˆ°æ­å·æ€ä¹ˆèµ°æœ€å¿«",
                "æˆ‘é™„è¿‘æœ‰ä»€ä¹ˆå¥½åƒçš„é¤å…",
                "æ·±åœ³å—å±±åŒºæœ‰ä»€ä¹ˆå¥½é…’åº—"
            ]
        }
    
    async def get_available_tools(self) -> Dict[str, Any]:
        """è·å–å¯ç”¨å·¥å…·ä¿¡æ¯"""
        try:
            async with MCPToolManager(AMAP_MCP_URL) as tool_manager:
                return {
                    "tools": tool_manager.available_tools,
                    "total_count": len(tool_manager.available_tools),
                    "descriptions": tool_manager.get_tools_description()
                }
        except Exception as e:
            return {"error": f"Failed to get tools: {e}"}
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        health_status = {
            "timestamp": datetime.now().isoformat(),
            "llm_available": False,
            "mcp_available": False,
            "overall_status": "unhealthy"
        }
        
        # æ£€æŸ¥LLM
        try:
            test_response = self.model.generate_content("æµ‹è¯•è¿æ¥")
            health_status["llm_available"] = True
        except Exception as e:
            health_status["llm_error"] = str(e)
        
        # æ£€æŸ¥MCPå·¥å…·
        try:
            async with MCPToolManager(AMAP_MCP_URL) as tool_manager:
                if tool_manager.available_tools:
                    health_status["mcp_available"] = True
                    health_status["mcp_tools_count"] = len(tool_manager.available_tools)
        except Exception as e:
            health_status["mcp_error"] = str(e)
        
        # ç»¼åˆçŠ¶æ€
        if health_status["llm_available"] and health_status["mcp_available"]:
            health_status["overall_status"] = "healthy"
        elif health_status["llm_available"]:
            health_status["overall_status"] = "degraded"
        
        return health_status

# ä½¿ç”¨ç¤ºä¾‹
async def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    analyzer = UniversalTravelAnalyzer()
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„æŸ¥è¯¢
    test_queries = [
        "æˆ‘åœ¨åŒ—äº¬æµ·æ·€åŒºå’Œæœé˜³åŒºéƒ½æœ‰å·¥ä½œï¼Œæƒ³æ‰¾ä¸€ä¸ªé€šå‹¤æ–¹ä¾¿çš„æˆ¿å­",
        "å¸®æˆ‘è§„åˆ’ä¸Šæµ·2å¤©æ—…æ¸¸æ”»ç•¥ï¼Œå–œæ¬¢å†å²æ–‡åŒ–",
        "ä»å¹¿å·åˆ°æ·±åœ³æœ€å¿«çš„äº¤é€šæ–¹å¼",
        "æˆ‘é™„è¿‘æœ‰ä»€ä¹ˆå¥½åƒçš„å·èœé¦†",
        "æ­å·è¥¿æ¹–é™„è¿‘æœ‰ä»€ä¹ˆå¥½é…’åº—"
    ]
    
    for query in test_queries:
        print(f"\n=== æŸ¥è¯¢: {query} ===")
        result = await analyzer.analyze_request(query)
        print(f"åˆ†æç±»å‹: {result.get('analysis_type')}")
        print(f"å·¥å…·è°ƒç”¨: {len(result.get('tool_calls', []))}æ¬¡")
        if 'final_response' in result:
            print(f"å“åº”: {result['final_response'][:200]}...")

if __name__ == "__main__":
    asyncio.run(example_usage())
